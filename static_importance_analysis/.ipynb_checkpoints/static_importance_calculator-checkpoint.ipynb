{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c89ab96",
   "metadata": {},
   "source": [
    "# Static Importance Index Calculator for Java Methods\n",
    "\n",
    "This notebook computes static importance indices for Java methods using both **Knowledge Graph** data from Neo4j and **AST** metadata. The goal is to create normalized weights that will be used for method retrieval in a hybrid RAG system for code generation.\n",
    "\n",
    "## Metrics Computed:\n",
    "- **Code Complexity**: LOC, Cyclomatic Complexity, Cognitive Complexity, Halstead Effort\n",
    "- **Graph Centrality**: Degree Centrality, Betweenness Centrality, Eigenvector Centrality\n",
    "- **Method Dependencies**: Fan-in, Fan-out \n",
    "- **Parameter Analysis**: Number of parameters, parameter type complexity, return type complexity\n",
    "\n",
    "## Data Sources:\n",
    "- **Neo4j Knowledge Graph**: `http://4.187.169.27:7474/browser/`\n",
    "- **AST Data**: `../AST/java_parsed.csv`\n",
    "- **Target Project**: Library Management System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc9c41c",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries\n",
    "\n",
    "Import all necessary libraries for Neo4j connectivity, data analysis, graph operations, and complexity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8304f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Tree-sitter available: True\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Neo4j connection\n",
    "from neo4j import GraphDatabase\n",
    "import logging\n",
    "\n",
    "# Graph analysis\n",
    "import networkx as nx\n",
    "\n",
    "# For complexity calculations\n",
    "import ast\n",
    "import math\n",
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "# For Java AST parsing\n",
    "try:\n",
    "    from tree_sitter import Language, Parser\n",
    "    from tree_sitter_languages import get_language\n",
    "    TREE_SITTER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"tree-sitter not available. Some complexity metrics will use simplified calculations.\")\n",
    "    TREE_SITTER_AVAILABLE = False\n",
    "\n",
    "# Setup plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Tree-sitter available: {TREE_SITTER_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9be61d",
   "metadata": {},
   "source": [
    "## 2. Connect to Neo4j Knowledge Graph\n",
    "\n",
    "Establish connection to the Neo4j database containing the Java knowledge graph and verify connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee96dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neo4j connection successful!\n",
      "Result: Connection successful\n",
      "\n",
      "üìä Database Statistics:\n",
      "Total nodes: 1,122\n",
      "Total relationships: 2,209\n",
      "Available node labels: ['Import', 'Package', 'Class', 'Field', 'Variable', 'Constructor', 'Parameter', 'Method', 'Type', 'Annotation', 'Interface', 'Enum']\n",
      "Available relationship types: ['HAS_FIELD', 'HAS_CONSTRUCTOR', 'HAS_PARAMETER', 'HAS_METHOD', 'BELONGS_TO', 'RETURNS', 'CALLS', 'CALLED_BY', 'USES', 'INHERITS', 'HAS_ANNOTATION', 'IMPLEMENTS']\n"
     ]
    }
   ],
   "source": [
    "# Neo4j connection configuration\n",
    "NEO4J_URI = \"bolt://4.187.169.27:7687\"  # Using bolt protocol\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"MyStrongPassword123\"\n",
    "\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "        \n",
    "    def query(self, query, parameters=None):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, parameters)\n",
    "            return [record for record in result]\n",
    "\n",
    "# Initialize connection\n",
    "neo4j_conn = Neo4jConnection(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "\n",
    "# Test connection and get database info\n",
    "try:\n",
    "    # Test basic connectivity\n",
    "    test_result = neo4j_conn.query(\"RETURN 'Connection successful' as message\")\n",
    "    print(\"‚úÖ Neo4j connection successful!\")\n",
    "    print(f\"Result: {test_result[0]['message']}\")\n",
    "    \n",
    "    # Get database statistics\n",
    "    node_count = neo4j_conn.query(\"MATCH (n) RETURN count(n) as count\")[0]['count']\n",
    "    rel_count = neo4j_conn.query(\"MATCH ()-[r]->() RETURN count(r) as count\")[0]['count']\n",
    "    \n",
    "    print(f\"\\nüìä Database Statistics:\")\n",
    "    print(f\"Total nodes: {node_count:,}\")\n",
    "    print(f\"Total relationships: {rel_count:,}\")\n",
    "    \n",
    "    # Get available node labels\n",
    "    labels_result = neo4j_conn.query(\"CALL db.labels()\")\n",
    "    labels = [record['label'] for record in labels_result]\n",
    "    print(f\"Available node labels: {labels}\")\n",
    "    \n",
    "    # Get available relationship types\n",
    "    rel_types_result = neo4j_conn.query(\"CALL db.relationshipTypes()\")\n",
    "    rel_types = [record['relationshipType'] for record in rel_types_result]\n",
    "    print(f\"Available relationship types: {rel_types}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"Please check the Neo4j server status and credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b25c1",
   "metadata": {},
   "source": [
    "## 3. Load AST Data from CSV\n",
    "\n",
    "Load the existing AST parsed data and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d3546c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded AST data: 264 methods found\n",
      "\n",
      "üìä AST Data Overview:\n",
      "Shape: (264, 10)\n",
      "Columns: ['FilePath', 'Package', 'Class', 'Method Name', 'Return Type', 'Parameters', 'Function Body', 'Throws', 'Modifiers', 'Generics']\n",
      "\n",
      "üîç Sample Data:\n",
      "                                            FilePath  Package       Class  \\\n",
      "0  C:\\Users\\divchauhan\\Downloads\\Library-Assistan...      NaN  AlertMaker   \n",
      "1  C:\\Users\\divchauhan\\Downloads\\Library-Assistan...      NaN  AlertMaker   \n",
      "2  C:\\Users\\divchauhan\\Downloads\\Library-Assistan...      NaN  AlertMaker   \n",
      "3  C:\\Users\\divchauhan\\Downloads\\Library-Assistan...      NaN  AlertMaker   \n",
      "4  C:\\Users\\divchauhan\\Downloads\\Library-Assistan...      NaN  AlertMaker   \n",
      "\n",
      "          Method Name Return Type  \\\n",
      "0     showSimpleAlert        void   \n",
      "1    showErrorMessage        void   \n",
      "2    showErrorMessage        void   \n",
      "3    showErrorMessage        void   \n",
      "4  showMaterialDialog        void   \n",
      "\n",
      "                                          Parameters  \\\n",
      "0                       String title, String content   \n",
      "1                       String title, String content   \n",
      "2                                       Exception ex   \n",
      "3         Exception ex, String title, String content   \n",
      "4  StackPane root, Node nodeToBeBlurred, List<JFX...   \n",
      "\n",
      "                                       Function Body Throws      Modifiers  \\\n",
      "0  {\\r\\n        Alert alert = new Alert(AlertType...    NaN  public static   \n",
      "1  {\\r\\n        Alert alert = new Alert(AlertType...    NaN  public static   \n",
      "2  {\\r\\n        Alert alert = new Alert(AlertType...    NaN  public static   \n",
      "3  {\\r\\n        Alert alert = new Alert(AlertType...    NaN  public static   \n",
      "4  {\\r\\n        BoxBlur blur = new BoxBlur(3, 3, ...    NaN  public static   \n",
      "\n",
      "   Generics  \n",
      "0       NaN  \n",
      "1       NaN  \n",
      "2       NaN  \n",
      "3       NaN  \n",
      "4       NaN  \n",
      "\n",
      "‚ùì Missing Values:\n",
      "Package          264\n",
      "Parameters       104\n",
      "Function Body      2\n",
      "Throws           246\n",
      "Modifiers          4\n",
      "Generics         264\n",
      "dtype: int64\n",
      "\n",
      "üìà Basic Statistics:\n",
      "Unique classes: 42\n",
      "Unique packages: 0\n",
      "Methods with function body: 262\n",
      "\n",
      "üèóÔ∏è Top 10 Classes by Method Count:\n",
      "Class\n",
      "MainController          31\n",
      "DatabaseHandler         18\n",
      "Book                    15\n",
      "Member                  12\n",
      "Preferences             11\n",
      "BookListController       9\n",
      "MemberListController     9\n",
      "EncryptionUtil           9\n",
      "SettingsController       9\n",
      "NotificationItem         9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load AST data from CSV\n",
    "ast_file_path = \"../AST/java_parsed.csv\"\n",
    "\n",
    "try:\n",
    "    ast_df = pd.read_csv(ast_file_path)\n",
    "    print(f\"‚úÖ Successfully loaded AST data: {len(ast_df)} methods found\")\n",
    "    \n",
    "    # Basic data exploration\n",
    "    print(f\"\\nüìä AST Data Overview:\")\n",
    "    print(f\"Shape: {ast_df.shape}\")\n",
    "    print(f\"Columns: {list(ast_df.columns)}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(f\"\\nüîç Sample Data:\")\n",
    "    print(ast_df.head())\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\n‚ùì Missing Values:\")\n",
    "    missing_counts = ast_df.isnull().sum()\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìà Basic Statistics:\")\n",
    "    print(f\"Unique classes: {ast_df['Class'].nunique()}\")\n",
    "    print(f\"Unique packages: {ast_df['Package'].nunique()}\")\n",
    "    print(f\"Methods with function body: {ast_df['Function Body'].notna().sum()}\")\n",
    "    \n",
    "    # Method distribution by class\n",
    "    method_counts = ast_df['Class'].value_counts()\n",
    "    print(f\"\\nüèóÔ∏è Top 10 Classes by Method Count:\")\n",
    "    print(method_counts.head(10))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Could not find AST file at: {ast_file_path}\")\n",
    "    print(\"Please ensure the AST parsing has been completed and the file exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading AST data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cc522",
   "metadata": {},
   "source": [
    "## 4. Extract Method Information from Knowledge Graph\n",
    "\n",
    "Query the Neo4j graph to extract method nodes and their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c2f9b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function. ('id' has been replaced by 'elementId or consider using an application-generated id')} {position: line: 4, column: 12, offset: 68} for query: '\\n    MATCH (m:Method)\\n    RETURN m.name as method_name, \\n           id(m) as node_id,\\n           m.depth as depth,\\n           labels(m) as labels\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Extracting method nodes...\n",
      "Found 280 method nodes\n",
      "üîÑ Extracting method relationships...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function. ('id' has been replaced by 'elementId or consider using an application-generated id')} {position: line: 6, column: 12, offset: 166} for query: '\\n    MATCH (m1:Method)-[r]->(m2:Method)\\n    RETURN m1.name as source_method,\\n           m2.name as target_method,\\n           type(r) as relationship_type,\\n           id(m1) as source_id,\\n           id(m2) as target_id\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function. ('id' has been replaced by 'elementId or consider using an application-generated id')} {position: line: 7, column: 12, offset: 198} for query: '\\n    MATCH (m1:Method)-[r]->(m2:Method)\\n    RETURN m1.name as source_method,\\n           m2.name as target_method,\\n           type(r) as relationship_type,\\n           id(m1) as source_id,\\n           id(m2) as target_id\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function. ('id' has been replaced by 'elementId or consider using an application-generated id')} {position: line: 5, column: 12, offset: 126} for query: '\\n    MATCH (c:Class)-[r:HAS_METHOD]->(m:Method)\\n    RETURN c.name as class_name,\\n           m.name as method_name,\\n           id(c) as class_id,\\n           id(m) as method_id\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function. ('id' has been replaced by 'elementId or consider using an application-generated id')} {position: line: 6, column: 12, offset: 156} for query: '\\n    MATCH (c:Class)-[r:HAS_METHOD]->(m:Method)\\n    RETURN c.name as class_name,\\n           m.name as method_name,\\n           id(c) as class_id,\\n           id(m) as method_id\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function. ('id' has been replaced by 'elementId or consider using an application-generated id')} {position: line: 5, column: 12, offset: 123} for query: '\\n    MATCH (m:Method)-[r:HAS_PARAMETER]->(p)\\n    RETURN m.name as method_name,\\n           p.name as param_name,\\n           id(m) as method_id,\\n           id(p) as param_id\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function. ('id' has been replaced by 'elementId or consider using an application-generated id')} {position: line: 6, column: 12, offset: 154} for query: '\\n    MATCH (m:Method)-[r:HAS_PARAMETER]->(p)\\n    RETURN m.name as method_name,\\n           p.name as param_name,\\n           id(m) as method_id,\\n           id(p) as param_id\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 744 method relationships\n",
      "üîÑ Extracting method-class relationships...\n",
      "Found 249 method-class relationships\n",
      "üîÑ Extracting method parameters...\n",
      "Found 0 method parameters\n",
      "\n",
      "‚úÖ Knowledge Graph data extracted successfully!\n",
      "\n",
      "üìä METHODS Sample:\n",
      "           method_name  node_id  depth    labels\n",
      "0      showSimpleAlert       27      1  [Method]\n",
      "1     showErrorMessage       33      2  [Method]\n",
      "2  getLocalizedMessage       36      0  [Method]\n",
      "3      printStackTrace       39      0  [Method]\n",
      "4             toString       41      1  [Method]\n",
      "\n",
      "üìä RELATIONSHIPS Sample:\n",
      "         source_method        target_method relationship_type  source_id  \\\n",
      "0  getLocalizedMessage     showErrorMessage         CALLED_BY         36   \n",
      "1             toString     showErrorMessage         CALLED_BY         41   \n",
      "2      printStackTrace     showErrorMessage         CALLED_BY         39   \n",
      "3           execAction  getLocalizedMessage             CALLS        247   \n",
      "4            execQuery  getLocalizedMessage             CALLS        241   \n",
      "\n",
      "   target_id  \n",
      "0         33  \n",
      "1         33  \n",
      "2         33  \n",
      "3         36  \n",
      "4         36  \n",
      "\n",
      "üìä METHOD_CLASS Sample:\n",
      "   class_name         method_name  class_id  method_id\n",
      "0  AlertMaker          styleAlert        26         72\n",
      "1  AlertMaker     showTrayMessage        26         60\n",
      "2  AlertMaker  showMaterialDialog        26         45\n",
      "3  AlertMaker    showErrorMessage        26         33\n",
      "4  AlertMaker     showSimpleAlert        26         27\n",
      "\n",
      "üìä METHOD_PARAMS Sample:\n",
      "No data found\n"
     ]
    }
   ],
   "source": [
    "# Extract method information from Knowledge Graph\n",
    "def extract_kg_data():\n",
    "    \"\"\"Extract all relevant data from the knowledge graph\"\"\"\n",
    "    \n",
    "    # Get all method nodes\n",
    "    methods_query = \"\"\"\n",
    "    MATCH (m:Method)\n",
    "    RETURN m.name as method_name, \n",
    "           id(m) as node_id,\n",
    "           m.depth as depth,\n",
    "           labels(m) as labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get method relationships (calls, uses, etc.)\n",
    "    relationships_query = \"\"\"\n",
    "    MATCH (m1:Method)-[r]->(m2:Method)\n",
    "    RETURN m1.name as source_method,\n",
    "           m2.name as target_method,\n",
    "           type(r) as relationship_type,\n",
    "           id(m1) as source_id,\n",
    "           id(m2) as target_id\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get method-class relationships\n",
    "    method_class_query = \"\"\"\n",
    "    MATCH (c:Class)-[r:HAS_METHOD]->(m:Method)\n",
    "    RETURN c.name as class_name,\n",
    "           m.name as method_name,\n",
    "           id(c) as class_id,\n",
    "           id(m) as method_id\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get method parameters\n",
    "    method_params_query = \"\"\"\n",
    "    MATCH (m:Method)-[r:HAS_PARAMETER]->(p)\n",
    "    RETURN m.name as method_name,\n",
    "           p.name as param_name,\n",
    "           id(m) as method_id,\n",
    "           id(p) as param_id\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üîÑ Extracting method nodes...\")\n",
    "        methods_data = neo4j_conn.query(methods_query)\n",
    "        methods_df = pd.DataFrame([dict(record) for record in methods_data])\n",
    "        print(f\"Found {len(methods_df)} method nodes\")\n",
    "        \n",
    "        print(\"üîÑ Extracting method relationships...\")\n",
    "        relationships_data = neo4j_conn.query(relationships_query)\n",
    "        relationships_df = pd.DataFrame([dict(record) for record in relationships_data])\n",
    "        print(f\"Found {len(relationships_df)} method relationships\")\n",
    "        \n",
    "        print(\"üîÑ Extracting method-class relationships...\")\n",
    "        method_class_data = neo4j_conn.query(method_class_query)\n",
    "        method_class_df = pd.DataFrame([dict(record) for record in method_class_data])\n",
    "        print(f\"Found {len(method_class_df)} method-class relationships\")\n",
    "        \n",
    "        print(\"üîÑ Extracting method parameters...\")\n",
    "        method_params_data = neo4j_conn.query(method_params_query)\n",
    "        method_params_df = pd.DataFrame([dict(record) for record in method_params_data])\n",
    "        print(f\"Found {len(method_params_df)} method parameters\")\n",
    "        \n",
    "        return {\n",
    "            'methods': methods_df,\n",
    "            'relationships': relationships_df,\n",
    "            'method_class': method_class_df,\n",
    "            'method_params': method_params_df\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting KG data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract the data\n",
    "kg_data = extract_kg_data()\n",
    "\n",
    "if kg_data:\n",
    "    print(\"\\n‚úÖ Knowledge Graph data extracted successfully!\")\n",
    "    \n",
    "    # Display sample data\n",
    "    for key, df in kg_data.items():\n",
    "        print(f\"\\nüìä {key.upper()} Sample:\")\n",
    "        if not df.empty:\n",
    "            print(df.head())\n",
    "        else:\n",
    "            print(\"No data found\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to extract knowledge graph data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dec1ca",
   "metadata": {},
   "source": [
    "## 5. Calculate Code Complexity Metrics\n",
    "\n",
    "Compute various complexity metrics for each method using the function body and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a013664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Calculating complexity metrics...\n",
      "‚úÖ Complexity metrics calculated!\n",
      "\n",
      "üìä Complexity Metrics Statistics:\n",
      "LOC: Mean=8.52, Max=49.00, Std=8.26\n",
      "Cyclomatic_Complexity: Mean=2.02, Max=10.00, Std=1.69\n",
      "Cognitive_Complexity: Mean=2.69, Max=41.00, Std=5.40\n",
      "Halstead_Effort: Mean=497.36, Max=11724.20, Std=1440.35\n",
      "Parameter_Count: Mean=0.78, Max=5.00, Std=0.84\n",
      "Parameter_Complexity: Mean=0.24, Max=6.00, Std=0.94\n",
      "Return_Type_Complexity: Mean=0.44, Max=4.00, Std=0.67\n",
      "\n",
      "üîç Sample Methods with Complexity Metrics:\n",
      "             Class         Method Name  LOC  Cyclomatic_Complexity  \\\n",
      "0       AlertMaker     showSimpleAlert    8                      1   \n",
      "1       AlertMaker    showErrorMessage    8                      1   \n",
      "2       AlertMaker    showErrorMessage   25                      1   \n",
      "3       AlertMaker    showErrorMessage   24                      1   \n",
      "4       AlertMaker  showMaterialDialog   22                      2   \n",
      "5       AlertMaker     showTrayMessage   14                      3   \n",
      "6       AlertMaker          styleAlert    7                      1   \n",
      "7  GenericCallback       taskCompleted    0                      1   \n",
      "8             Book               getId    3                      1   \n",
      "9             Book               setId    3                      1   \n",
      "\n",
      "   Cognitive_Complexity  Halstead_Effort  \n",
      "0                     2        55.645957  \n",
      "1                     0        55.645957  \n",
      "2                     0       572.234268  \n",
      "3                     0       535.590434  \n",
      "4                     6      1087.588945  \n",
      "5                     3       170.248302  \n",
      "6                     2       341.956683  \n",
      "7                     0         0.000000  \n",
      "8                     0         0.000000  \n",
      "9                     0         4.754888  \n"
     ]
    }
   ],
   "source": [
    "class ComplexityCalculator:\n",
    "    \"\"\"Calculate various complexity metrics for Java methods\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Java keywords that increase cyclomatic complexity\n",
    "        self.decision_keywords = [\n",
    "            'if', 'else', 'elif', 'while', 'for', 'switch', 'case', \n",
    "            'catch', 'try', '&&', '||', '?', 'do'\n",
    "        ]\n",
    "        \n",
    "    def calculate_lines_of_code(self, function_body):\n",
    "        \"\"\"Calculate Lines of Code (LOC)\"\"\"\n",
    "        if pd.isna(function_body) or function_body.strip() == \"\":\n",
    "            return 0\n",
    "        \n",
    "        # Remove empty lines and comments\n",
    "        lines = function_body.split('\\n')\n",
    "        non_empty_lines = [line.strip() for line in lines if line.strip() and not line.strip().startswith('//')]\n",
    "        return len(non_empty_lines)\n",
    "    \n",
    "    def calculate_cyclomatic_complexity(self, function_body):\n",
    "        \"\"\"Calculate Cyclomatic Complexity (simplified version)\"\"\"\n",
    "        if pd.isna(function_body) or function_body.strip() == \"\":\n",
    "            return 1  # Base complexity\n",
    "        \n",
    "        complexity = 1  # Base complexity\n",
    "        \n",
    "        # Count decision points\n",
    "        for keyword in self.decision_keywords:\n",
    "            if keyword in ['&&', '||']:\n",
    "                complexity += function_body.count(keyword)\n",
    "            else:\n",
    "                # Use word boundaries for keywords\n",
    "                import re\n",
    "                pattern = r'\\b' + re.escape(keyword) + r'\\b'\n",
    "                complexity += len(re.findall(pattern, function_body, re.IGNORECASE))\n",
    "        \n",
    "        return complexity\n",
    "    \n",
    "    def calculate_cognitive_complexity(self, function_body):\n",
    "        \"\"\"Calculate Cognitive Complexity (simplified)\"\"\"\n",
    "        if pd.isna(function_body) or function_body.strip() == \"\":\n",
    "            return 0\n",
    "        \n",
    "        cognitive = 0\n",
    "        nesting_level = 0\n",
    "        \n",
    "        # Simple nesting and branching detection\n",
    "        lines = function_body.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Increase nesting for blocks\n",
    "            if '{' in line:\n",
    "                nesting_level += line.count('{')\n",
    "            if '}' in line:\n",
    "                nesting_level -= line.count('}')\n",
    "                nesting_level = max(0, nesting_level)\n",
    "            \n",
    "            # Add complexity based on constructs\n",
    "            for keyword in ['if', 'while', 'for', 'switch', 'catch']:\n",
    "                if keyword in line.lower():\n",
    "                    cognitive += 1 + nesting_level\n",
    "                    \n",
    "        return cognitive\n",
    "    \n",
    "    def calculate_halstead_metrics(self, function_body):\n",
    "        \"\"\"Calculate Halstead metrics (simplified)\"\"\"\n",
    "        if pd.isna(function_body) or function_body.strip() == \"\":\n",
    "            return {'volume': 0, 'difficulty': 0, 'effort': 0}\n",
    "        \n",
    "        # Java operators and keywords\n",
    "        operators = ['+', '-', '*', '/', '%', '=', '==', '!=', '<', '>', '<=', '>=', \n",
    "                    '&&', '||', '!', '++', '--', '+=', '-=', '*=', '/=']\n",
    "        \n",
    "        # Count unique and total operators/operands\n",
    "        unique_operators = set()\n",
    "        total_operators = 0\n",
    "        unique_operands = set()\n",
    "        total_operands = 0\n",
    "        \n",
    "        # Simple tokenization (could be improved with proper parsing)\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[+\\-*/=<>!&|%]+', function_body)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in operators or any(op in token for op in operators):\n",
    "                unique_operators.add(token)\n",
    "                total_operators += 1\n",
    "            else:\n",
    "                unique_operands.add(token)\n",
    "                total_operands += 1\n",
    "        \n",
    "        # Halstead metrics\n",
    "        n1 = len(unique_operators)  # Number of distinct operators\n",
    "        n2 = len(unique_operands)   # Number of distinct operands\n",
    "        N1 = total_operators        # Total number of operators\n",
    "        N2 = total_operands         # Total number of operands\n",
    "        \n",
    "        if n1 == 0 or n2 == 0:\n",
    "            return {'volume': 0, 'difficulty': 0, 'effort': 0}\n",
    "        \n",
    "        vocabulary = n1 + n2\n",
    "        length = N1 + N2\n",
    "        volume = length * math.log2(vocabulary) if vocabulary > 1 else 0\n",
    "        difficulty = (n1 / 2) * (N2 / n2) if n2 > 0 else 0\n",
    "        effort = difficulty * volume\n",
    "        \n",
    "        return {\n",
    "            'volume': volume,\n",
    "            'difficulty': difficulty,\n",
    "            'effort': effort\n",
    "        }\n",
    "    \n",
    "    def count_parameters(self, parameters):\n",
    "        \"\"\"Count number of parameters\"\"\"\n",
    "        if pd.isna(parameters) or parameters.strip() == \"\":\n",
    "            return 0\n",
    "        \n",
    "        # Simple parameter counting\n",
    "        if parameters.strip() == \"\":\n",
    "            return 0\n",
    "        \n",
    "        # Split by comma and count non-empty parts\n",
    "        params = [p.strip() for p in parameters.split(',') if p.strip()]\n",
    "        return len(params)\n",
    "    \n",
    "    def calculate_parameter_complexity(self, parameters):\n",
    "        \"\"\"Calculate parameter type complexity\"\"\"\n",
    "        if pd.isna(parameters) or parameters.strip() == \"\":\n",
    "            return 0\n",
    "        \n",
    "        complexity = 0\n",
    "        \n",
    "        # Complex types add more complexity\n",
    "        complex_types = ['List', 'Map', 'Set', 'Collection', 'Array', '[]', '<', '>']\n",
    "        generic_indicators = ['<', '>', 'List', 'Map', 'Set']\n",
    "        \n",
    "        for complex_type in complex_types:\n",
    "            complexity += parameters.count(complex_type)\n",
    "        \n",
    "        # Generics add extra complexity\n",
    "        if any(indicator in parameters for indicator in generic_indicators):\n",
    "            complexity += 2\n",
    "            \n",
    "        return complexity\n",
    "    \n",
    "    def calculate_return_type_complexity(self, return_type):\n",
    "        \"\"\"Calculate return type complexity\"\"\"\n",
    "        if pd.isna(return_type) or return_type.strip() == \"\":\n",
    "            return 0\n",
    "        \n",
    "        complexity = 1  # Base complexity for having a return type\n",
    "        \n",
    "        # Void methods have 0 complexity\n",
    "        if return_type.lower() == 'void':\n",
    "            return 0\n",
    "        \n",
    "        # Complex return types\n",
    "        complex_indicators = ['List', 'Map', 'Set', 'Collection', '[]', '<', '>']\n",
    "        for indicator in complex_indicators:\n",
    "            if indicator in return_type:\n",
    "                complexity += 1\n",
    "        \n",
    "        return complexity\n",
    "\n",
    "# Initialize calculator\n",
    "complexity_calc = ComplexityCalculator()\n",
    "\n",
    "# Calculate complexity metrics for all methods\n",
    "print(\"üîÑ Calculating complexity metrics...\")\n",
    "\n",
    "# Create a copy of AST dataframe for processing\n",
    "enhanced_df = ast_df.copy()\n",
    "\n",
    "# Calculate all complexity metrics\n",
    "enhanced_df['LOC'] = enhanced_df['Function Body'].apply(complexity_calc.calculate_lines_of_code)\n",
    "enhanced_df['Cyclomatic_Complexity'] = enhanced_df['Function Body'].apply(complexity_calc.calculate_cyclomatic_complexity)\n",
    "enhanced_df['Cognitive_Complexity'] = enhanced_df['Function Body'].apply(complexity_calc.calculate_cognitive_complexity)\n",
    "\n",
    "# Calculate Halstead metrics\n",
    "halstead_metrics = enhanced_df['Function Body'].apply(complexity_calc.calculate_halstead_metrics)\n",
    "enhanced_df['Halstead_Volume'] = [h['volume'] for h in halstead_metrics]\n",
    "enhanced_df['Halstead_Difficulty'] = [h['difficulty'] for h in halstead_metrics]\n",
    "enhanced_df['Halstead_Effort'] = [h['effort'] for h in halstead_metrics]\n",
    "\n",
    "# Parameter analysis\n",
    "enhanced_df['Parameter_Count'] = enhanced_df['Parameters'].apply(complexity_calc.count_parameters)\n",
    "enhanced_df['Parameter_Complexity'] = enhanced_df['Parameters'].apply(complexity_calc.calculate_parameter_complexity)\n",
    "enhanced_df['Return_Type_Complexity'] = enhanced_df['Return Type'].apply(complexity_calc.calculate_return_type_complexity)\n",
    "\n",
    "print(\"‚úÖ Complexity metrics calculated!\")\n",
    "\n",
    "# Display statistics\n",
    "print(f\"\\nüìä Complexity Metrics Statistics:\")\n",
    "complexity_columns = ['LOC', 'Cyclomatic_Complexity', 'Cognitive_Complexity', \n",
    "                     'Halstead_Effort', 'Parameter_Count', 'Parameter_Complexity', 'Return_Type_Complexity']\n",
    "\n",
    "for col in complexity_columns:\n",
    "    print(f\"{col}: Mean={enhanced_df[col].mean():.2f}, Max={enhanced_df[col].max():.2f}, Std={enhanced_df[col].std():.2f}\")\n",
    "\n",
    "# Show sample with complexity metrics\n",
    "print(f\"\\nüîç Sample Methods with Complexity Metrics:\")\n",
    "sample_cols = ['Class', 'Method Name', 'LOC', 'Cyclomatic_Complexity', 'Cognitive_Complexity', 'Halstead_Effort']\n",
    "print(enhanced_df[sample_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45884bf0",
   "metadata": {},
   "source": [
    "## 6. Compute Graph Centrality Measures\n",
    "\n",
    "Calculate centrality metrics using the method call graph from the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f785c170",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (2117252320.py, line 61)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 61\u001b[1;36m\u001b[0m\n\u001b[1;33m    betweenness_centrality = {node: 0.0 for node in G.nodes()}\\n\u001b[0m\n\u001b[1;37m                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "def compute_centrality_measures(kg_data, enhanced_df):\n",
    "    \"\"\"Compute centrality measures from the knowledge graph\"\"\"\n",
    "    \n",
    "    if not kg_data or kg_data['relationships'].empty:\n",
    "        print(\"‚ö†Ô∏è No relationship data available for centrality calculation\")\n",
    "        # Return default values\n",
    "        default_centrality = pd.DataFrame({\n",
    "            'method_name': enhanced_df['Method Name'],\n",
    "            'degree_centrality': 0.0,\n",
    "            'betweenness_centrality': 0.0,\n",
    "            'eigenvector_centrality': 0.0,\n",
    "            'fan_in': 0,\n",
    "            'fan_out': 0\n",
    "        })\n",
    "        return default_centrality\n",
    "    \n",
    "    # Create a directed graph from relationships\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add method nodes\n",
    "    all_methods = set()\n",
    "    if not kg_data['methods'].empty:\n",
    "        method_names = kg_data['methods']['method_name'].tolist()\n",
    "        all_methods.update(method_names)\n",
    "    \n",
    "    # Add methods from AST data\n",
    "    ast_methods = enhanced_df['Method Name'].tolist()\n",
    "    all_methods.update(ast_methods)\n",
    "    \n",
    "    # Add all methods as nodes\n",
    "    G.add_nodes_from(all_methods)\n",
    "    \n",
    "    # Add edges from relationships\n",
    "    relationships_df = kg_data['relationships']\n",
    "    \n",
    "    for _, row in relationships_df.iterrows():\n",
    "        source = row['source_method']\n",
    "        target = row['target_method']\n",
    "        rel_type = row['relationship_type']\n",
    "        \n",
    "        # Add edge with relationship type as attribute\n",
    "        if source and target:\n",
    "            G.add_edge(source, target, relationship=rel_type)\n",
    "    \n",
    "    print(f\"üìä Graph Statistics:\")\n",
    "    print(f\"Nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Edges: {G.number_of_edges()}\")\n",
    "    print(f\"Is connected: {nx.is_connected(G.to_undirected())}\")\n",
    "    \n",
    "    # Calculate centrality measures\n",
    "    print(\"üîÑ Calculating centrality measures...\")\n",
    "    \n",
    "    # Degree centrality\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    \n",
    "    # Betweenness centrality (can be slow for large graphs)\n",
    "    try:\n",
    "        betweenness_centrality = nx.betweenness_centrality(G, k=min(100, G.number_of_nodes()))\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Using approximate betweenness centrality\")\n",
    "        betweenness_centrality = {node: 0.0 for node in G.nodes()}    \n",
    "    # Eigenvector centrality\n",
    "    try:\n",
    "        eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Eigenvector centrality failed, using degree centrality as proxy\")\n",
    "        eigenvector_centrality = degree_centrality.copy()\n",
    "    \n",
    "    # Fan-in and Fan-out\n",
    "    fan_in = {node: G.in_degree(node) for node in G.nodes()}\n",
    "    fan_out = {node: G.out_degree(node) for node in G.nodes()}\n",
    "    \n",
    "    # Create centrality dataframe\n",
    "    centrality_data = []\n",
    "    for method in all_methods:\n",
    "        centrality_data.append({\n",
    "            'method_name': method,\n",
    "            'degree_centrality': degree_centrality.get(method, 0.0),\n",
    "            'betweenness_centrality': betweenness_centrality.get(method, 0.0),\n",
    "            'eigenvector_centrality': eigenvector_centrality.get(method, 0.0),\n",
    "            'fan_in': fan_in.get(method, 0),\n",
    "            'fan_out': fan_out.get(method, 0)\n",
    "        })\n",
    "    \n",
    "    centrality_df = pd.DataFrame(centrality_data)\n",
    "    \n",
    "    return centrality_df\n",
    "\n",
    "# Compute centrality measures\n",
    "print(\"üîÑ Computing graph centrality measures...\")\n",
    "centrality_df = compute_centrality_measures(kg_data, enhanced_df)\n",
    "\n",
    "print(\"‚úÖ Centrality measures computed!\")\n",
    "\n",
    "# Display centrality statistics\n",
    "print(f\"\\nüìä Centrality Statistics:\")\n",
    "centrality_cols = ['degree_centrality', 'betweenness_centrality', 'eigenvector_centrality', 'fan_in', 'fan_out']\n",
    "\n",
    "for col in centrality_cols:\n",
    "    mean_val = centrality_df[col].mean()\n",
    "    max_val = centrality_df[col].max()\n",
    "    std_val = centrality_df[col].std()\n",
    "    print(f\"{col}: Mean={mean_val:.4f}, Max={max_val:.4f}, Std={std_val:.4f}\")\n",
    "\n",
    "# Show top methods by different centrality measures\n",
    "print(f\"\\nüèÜ Top Methods by Centrality:\")\n",
    "\n",
    "for measure in ['degree_centrality', 'betweenness_centrality', 'eigenvector_centrality']:\n",
    "    print(f\"\\nTop 5 by {measure}:\")\n",
    "    top_methods = centrality_df.nlargest(5, measure)[['method_name', measure]]\n",
    "    print(top_methods)\n",
    "\n",
    "# Merge centrality data with enhanced dataframe\n",
    "enhanced_df = enhanced_df.merge(\n",
    "    centrality_df, \n",
    "    left_on='Method Name', \n",
    "    right_on='method_name', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing centrality values with 0\n",
    "centrality_columns = ['degree_centrality', 'betweenness_centrality', 'eigenvector_centrality', 'fan_in', 'fan_out']\n",
    "for col in centrality_columns:\n",
    "    enhanced_df[col] = enhanced_df[col].fillna(0)\n",
    "\n",
    "print(f\"\\n‚úÖ Centrality measures merged with method data!\")\n",
    "print(f\"Enhanced dataset shape: {enhanced_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9bd89",
   "metadata": {},
   "source": [
    "## 7. Calculate Static Importance Weights\n",
    "\n",
    "Combine all metrics using weighted scoring and normalize the values to create final importance indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3588c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticImportanceCalculator:\n",
    "    \"\"\"Calculate static importance weights for methods\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define weights for different metric categories\n",
    "        self.weights = {\n",
    "            # Code Complexity Metrics (40% total weight)\n",
    "            'LOC': 0.08,\n",
    "            'Cyclomatic_Complexity': 0.12,\n",
    "            'Cognitive_Complexity': 0.10,\n",
    "            'Halstead_Effort': 0.10,\n",
    "            \n",
    "            # Graph Centrality Metrics (35% total weight)\n",
    "            'degree_centrality': 0.10,\n",
    "            'betweenness_centrality': 0.10,\n",
    "            'eigenvector_centrality': 0.08,\n",
    "            'fan_in': 0.04,\n",
    "            'fan_out': 0.03,\n",
    "            \n",
    "            # Parameter and Interface Metrics (25% total weight)\n",
    "            'Parameter_Count': 0.08,\n",
    "            'Parameter_Complexity': 0.09,\n",
    "            'Return_Type_Complexity': 0.08\n",
    "        }\n",
    "        \n",
    "        # Verify weights sum to 1.0\n",
    "        total_weight = sum(self.weights.values())\n",
    "        if abs(total_weight - 1.0) > 0.01:\n",
    "            print(f\"‚ö†Ô∏è Warning: Weights sum to {total_weight}, not 1.0\")\n",
    "    \n",
    "    def normalize_column(self, series, method='min-max'):\n",
    "        \"\"\"Normalize a pandas series to 0-1 range\"\"\"\n",
    "        if method == 'min-max':\n",
    "            min_val = series.min()\n",
    "            max_val = series.max()\n",
    "            if max_val == min_val:\n",
    "                return pd.Series([0.5] * len(series), index=series.index)\n",
    "            return (series - min_val) / (max_val - min_val)\n",
    "        \n",
    "        elif method == 'z-score':\n",
    "            return (series - series.mean()) / series.std()\n",
    "        \n",
    "        elif method == 'robust':\n",
    "            median = series.median()\n",
    "            mad = (series - median).abs().median()\n",
    "            if mad == 0:\n",
    "                return pd.Series([0.5] * len(series), index=series.index)\n",
    "            return (series - median) / (1.4826 * mad)\n",
    "    \n",
    "    def calculate_importance_scores(self, df):\n",
    "        \"\"\"Calculate static importance scores for all methods\"\"\"\n",
    "        \n",
    "        # Create a copy for processing\n",
    "        scoring_df = df.copy()\n",
    "        \n",
    "        # Normalize all metrics to 0-1 range\n",
    "        normalized_metrics = {}\n",
    "        \n",
    "        for metric, weight in self.weights.items():\n",
    "            if metric in scoring_df.columns:\n",
    "                # Handle special cases\n",
    "                if metric in ['LOC', 'Cyclomatic_Complexity', 'Cognitive_Complexity', 'Halstead_Effort']:\n",
    "                    # Higher values = higher importance\n",
    "                    normalized = self.normalize_column(scoring_df[metric], 'min-max')\n",
    "                elif metric in ['degree_centrality', 'betweenness_centrality', 'eigenvector_centrality']:\n",
    "                    # Higher centrality = higher importance\n",
    "                    normalized = self.normalize_column(scoring_df[metric], 'min-max')\n",
    "                elif metric in ['fan_in', 'fan_out']:\n",
    "                    # Higher connectivity = higher importance (but cap extreme values)\n",
    "                    capped_values = np.minimum(scoring_df[metric], scoring_df[metric].quantile(0.95))\n",
    "                    normalized = self.normalize_column(capped_values, 'min-max')\n",
    "                else:\n",
    "                    # Default normalization\n",
    "                    normalized = self.normalize_column(scoring_df[metric], 'min-max')\n",
    "                \n",
    "                normalized_metrics[f'{metric}_normalized'] = normalized\n",
    "                \n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Warning: Metric '{metric}' not found in dataframe\")\n",
    "                normalized_metrics[f'{metric}_normalized'] = pd.Series([0.0] * len(scoring_df))\n",
    "        \n",
    "        # Calculate weighted importance score\n",
    "        importance_scores = pd.Series([0.0] * len(scoring_df), index=scoring_df.index)\n",
    "        \n",
    "        for metric, weight in self.weights.items():\n",
    "            normalized_col = f'{metric}_normalized'\n",
    "            if normalized_col in normalized_metrics:\n",
    "                importance_scores += normalized_metrics[normalized_col] * weight\n",
    "        \n",
    "        # Normalize final scores to 0-1 range\n",
    "        final_scores = self.normalize_column(importance_scores, 'min-max')\n",
    "        \n",
    "        # Add normalized metrics and scores to dataframe\n",
    "        for col, values in normalized_metrics.items():\n",
    "            scoring_df[col] = values\n",
    "        \n",
    "        scoring_df['importance_score_raw'] = importance_scores\n",
    "        scoring_df['importance_score_normalized'] = final_scores\n",
    "        \n",
    "        return scoring_df\n",
    "    \n",
    "    def categorize_importance(self, scores):\n",
    "        \"\"\"Categorize methods by importance level\"\"\"\n",
    "        categories = []\n",
    "        \n",
    "        for score in scores:\n",
    "            if score >= 0.8:\n",
    "                categories.append('Critical')\n",
    "            elif score >= 0.6:\n",
    "                categories.append('High')\n",
    "            elif score >= 0.4:\n",
    "                categories.append('Medium')\n",
    "            elif score >= 0.2:\n",
    "                categories.append('Low')\n",
    "            else:\n",
    "                categories.append('Minimal')\n",
    "        \n",
    "        return categories\n",
    "\n",
    "# Initialize importance calculator\n",
    "importance_calc = StaticImportanceCalculator()\n",
    "\n",
    "print(\"üîÑ Calculating static importance weights...\")\n",
    "\n",
    "# Calculate importance scores\n",
    "final_df = importance_calc.calculate_importance_scores(enhanced_df)\n",
    "\n",
    "# Add importance categories\n",
    "final_df['importance_category'] = importance_calc.categorize_importance(final_df['importance_score_normalized'])\n",
    "\n",
    "print(\"‚úÖ Static importance weights calculated!\")\n",
    "\n",
    "# Display weight configuration\n",
    "print(f\"\\n‚öñÔ∏è Weight Configuration:\")\n",
    "for metric, weight in importance_calc.weights.items():\n",
    "    print(f\"{metric}: {weight:.3f} ({weight*100:.1f}%)\")\n",
    "\n",
    "# Display importance statistics\n",
    "print(f\"\\nüìä Importance Score Statistics:\")\n",
    "print(f\"Mean: {final_df['importance_score_normalized'].mean():.4f}\")\n",
    "print(f\"Std: {final_df['importance_score_normalized'].std():.4f}\")\n",
    "print(f\"Min: {final_df['importance_score_normalized'].min():.4f}\")\n",
    "print(f\"Max: {final_df['importance_score_normalized'].max():.4f}\")\n",
    "\n",
    "# Show distribution by category\n",
    "print(f\"\\nüìà Importance Category Distribution:\")\n",
    "category_counts = final_df['importance_category'].value_counts()\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / len(final_df)) * 100\n",
    "    print(f\"{category}: {count} methods ({percentage:.1f}%)\")\n",
    "\n",
    "# Show top methods by importance\n",
    "print(f\"\\nüèÜ Top 10 Most Important Methods:\")\n",
    "top_methods = final_df.nlargest(10, 'importance_score_normalized')[\n",
    "    ['Class', 'Method Name', 'importance_score_normalized', 'importance_category', \n",
    "     'LOC', 'Cyclomatic_Complexity', 'degree_centrality']\n",
    "]\n",
    "print(top_methods)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Importance score distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(final_df['importance_score_normalized'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Importance Scores')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot 2: Category distribution\n",
    "plt.subplot(2, 3, 2)\n",
    "category_counts.plot(kind='bar', alpha=0.7)\n",
    "plt.title('Methods by Importance Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot 3: Correlation between metrics\n",
    "plt.subplot(2, 3, 3)\n",
    "correlation_metrics = ['LOC', 'Cyclomatic_Complexity', 'degree_centrality', 'importance_score_normalized']\n",
    "corr_data = final_df[correlation_metrics].corr()\n",
    "sns.heatmap(corr_data, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Metric Correlations')\n",
    "\n",
    "# Plot 4: Complexity vs Centrality\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(final_df['Cyclomatic_Complexity'], final_df['degree_centrality'], \n",
    "           c=final_df['importance_score_normalized'], cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(label='Importance Score')\n",
    "plt.xlabel('Cyclomatic Complexity')\n",
    "plt.ylabel('Degree Centrality')\n",
    "plt.title('Complexity vs Centrality')\n",
    "\n",
    "# Plot 5: LOC vs Importance\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(final_df['LOC'], final_df['importance_score_normalized'], alpha=0.6)\n",
    "plt.xlabel('Lines of Code')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('LOC vs Importance Score')\n",
    "\n",
    "# Plot 6: Top methods bar chart\n",
    "plt.subplot(2, 3, 6)\n",
    "top_10 = final_df.nlargest(10, 'importance_score_normalized')\n",
    "y_pos = np.arange(len(top_10))\n",
    "plt.barh(y_pos, top_10['importance_score_normalized'])\n",
    "plt.yticks(y_pos, [f\"{row['Class']}.{row['Method Name']}\"[:20] + \"...\" \n",
    "                   if len(f\"{row['Class']}.{row['Method Name']}\") > 20 \n",
    "                   else f\"{row['Class']}.{row['Method Name']}\" \n",
    "                   for _, row in top_10.iterrows()])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Top 10 Methods by Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìã Final dataset shape: {final_df.shape}\")\n",
    "print(f\"üìã Total columns: {len(final_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb652fd5",
   "metadata": {},
   "source": [
    "## 8. Export Enhanced Dataset\n",
    "\n",
    "Save the enhanced dataset with all original AST data plus new complexity metrics and normalized importance weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export enhanced dataset\n",
    "output_file = \"enhanced_java_methods_with_importance.csv\"\n",
    "\n",
    "# Select columns for export (organized by category)\n",
    "original_columns = [\n",
    "    'FilePath', 'Package', 'Class', 'Method Name', 'Return Type', \n",
    "    'Parameters', 'Function Body', 'Throws', 'Modifiers', 'Generics'\n",
    "]\n",
    "\n",
    "complexity_columns = [\n",
    "    'LOC', 'Cyclomatic_Complexity', 'Cognitive_Complexity',\n",
    "    'Halstead_Volume', 'Halstead_Difficulty', 'Halstead_Effort',\n",
    "    'Parameter_Count', 'Parameter_Complexity', 'Return_Type_Complexity'\n",
    "]\n",
    "\n",
    "centrality_columns = [\n",
    "    'degree_centrality', 'betweenness_centrality', 'eigenvector_centrality',\n",
    "    'fan_in', 'fan_out'\n",
    "]\n",
    "\n",
    "importance_columns = [\n",
    "    'importance_score_raw', 'importance_score_normalized', 'importance_category'\n",
    "]\n",
    "\n",
    "# Combine all columns for export\n",
    "export_columns = original_columns + complexity_columns + centrality_columns + importance_columns\n",
    "\n",
    "# Create export dataframe\n",
    "export_df = final_df[export_columns].copy()\n",
    "\n",
    "# Remove the 'method_name' column if it exists (duplicate of 'Method Name')\n",
    "if 'method_name' in export_df.columns:\n",
    "    export_df = export_df.drop('method_name', axis=1)\n",
    "\n",
    "# Sort by importance score (descending)\n",
    "export_df = export_df.sort_values('importance_score_normalized', ascending=False)\n",
    "\n",
    "# Save to CSV\n",
    "try:\n",
    "    export_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Enhanced dataset exported to: {output_file}\")\n",
    "    print(f\"üìä Exported {len(export_df)} methods with {len(export_df.columns)} features\")\n",
    "    \n",
    "    # Display file info\n",
    "    import os\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"üìÅ File size: {file_size:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error exporting dataset: {e}\")\n",
    "\n",
    "# Create summary statistics file\n",
    "summary_stats = {\n",
    "    'Dataset Overview': {\n",
    "        'Total Methods': len(export_df),\n",
    "        'Total Features': len(export_df.columns),\n",
    "        'Classes Analyzed': export_df['Class'].nunique(),\n",
    "        'Packages Analyzed': export_df['Package'].nunique()\n",
    "    },\n",
    "    'Complexity Metrics Summary': {\n",
    "        'Avg LOC': export_df['LOC'].mean(),\n",
    "        'Avg Cyclomatic Complexity': export_df['Cyclomatic_Complexity'].mean(),\n",
    "        'Avg Cognitive Complexity': export_df['Cognitive_Complexity'].mean(),\n",
    "        'Avg Halstead Effort': export_df['Halstead_Effort'].mean()\n",
    "    },\n",
    "    'Centrality Metrics Summary': {\n",
    "        'Avg Degree Centrality': export_df['degree_centrality'].mean(),\n",
    "        'Avg Betweenness Centrality': export_df['betweenness_centrality'].mean(),\n",
    "        'Avg Eigenvector Centrality': export_df['eigenvector_centrality'].mean(),\n",
    "        'Avg Fan-in': export_df['fan_in'].mean(),\n",
    "        'Avg Fan-out': export_df['fan_out'].mean()\n",
    "    },\n",
    "    'Importance Distribution': {\n",
    "        'Critical Methods': len(export_df[export_df['importance_category'] == 'Critical']),\n",
    "        'High Importance': len(export_df[export_df['importance_category'] == 'High']),\n",
    "        'Medium Importance': len(export_df[export_df['importance_category'] == 'Medium']),\n",
    "        'Low Importance': len(export_df[export_df['importance_category'] == 'Low']),\n",
    "        'Minimal Importance': len(export_df[export_df['importance_category'] == 'Minimal'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary statistics\n",
    "import json\n",
    "summary_file = \"analysis_summary.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Summary statistics saved to: {summary_file}\")\n",
    "\n",
    "# Display column information\n",
    "print(f\"\\nüìã Exported Dataset Columns:\")\n",
    "print(f\"\\nüî§ Original AST Columns ({len(original_columns)}):\")\n",
    "for col in original_columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nüßÆ Complexity Metrics ({len(complexity_columns)}):\")\n",
    "for col in complexity_columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nüï∏Ô∏è Centrality Metrics ({len(centrality_columns)}):\")\n",
    "for col in centrality_columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\n‚≠ê Importance Metrics ({len(importance_columns)}):\")\n",
    "for col in importance_columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Show sample of final export data\n",
    "print(f\"\\nüîç Sample of Enhanced Dataset:\")\n",
    "sample_cols = ['Class', 'Method Name', 'LOC', 'Cyclomatic_Complexity', \n",
    "               'degree_centrality', 'importance_score_normalized', 'importance_category']\n",
    "print(export_df[sample_cols].head(10))\n",
    "\n",
    "# Close Neo4j connection\n",
    "try:\n",
    "    neo4j_conn.close()\n",
    "    print(f\"\\n‚úÖ Neo4j connection closed successfully\")\n",
    "except:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Could not close Neo4j connection\")\n",
    "\n",
    "print(f\"\\nüéâ Static Importance Analysis Complete!\")\n",
    "print(f\"üìÅ Output files:\")\n",
    "print(f\"  - {output_file} (Enhanced dataset)\")\n",
    "print(f\"  - {summary_file} (Analysis summary)\")\n",
    "print(f\"\\nüí° The importance weights can now be used for method retrieval in your hybrid RAG system!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
